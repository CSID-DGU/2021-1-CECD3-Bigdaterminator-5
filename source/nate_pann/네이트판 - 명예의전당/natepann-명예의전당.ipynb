{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import csv\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from konlpy.tag import Komoran, Okt\n",
    "from nltk import Text\n",
    "from pykospacing import spacing\n",
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작날짜 입력(eg.20210920):20210920\n"
     ]
    }
   ],
   "source": [
    "browser = Chrome()\n",
    "\n",
    "titles = []\n",
    "links = []\n",
    "\n",
    "date = input(\"시작날짜 입력(eg.20210920):\")\n",
    "\n",
    "#링크, 제목\n",
    "for i in range(0,5):\n",
    "    \n",
    "    for j in range(1,3):\n",
    "        browser.get(\"https://pann.nate.com/talk/ranking/d?stdt=\" + str(date) + \"&page=\" + str(j))\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "        first_list = soup.find('div', {'class': 'cntList'}).findAll('li')\n",
    "\n",
    "    for li in first_list: \n",
    "        f_link = li.findAll('a')\n",
    "        for a in f_link:\n",
    "            real_link = 'https://pann.nate.com' + a.get('href') \n",
    "        links.append(real_link)\n",
    "    \n",
    "    for li in first_list:\n",
    "        f_title = li.findAll('dl')\n",
    "        for dl in f_title:\n",
    "            t = dl.find('a')\n",
    "            real_title = t.get('title')\n",
    "            real_title = spacing(real_title)\n",
    "            real_title = re.sub('[ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', '', real_title)\n",
    "            real_title = re.sub('[\\xa0|0xed]', '', real_title)\n",
    "            real_title = re.sub('[-=+_★♥♡,#/\\?:╋^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》—;]','',real_title)\n",
    "        titles.append(real_title)\n",
    "        \n",
    "    date = int(date) + 1\n",
    "\n",
    "#print({'제목': titles, 'url': links}, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#본문\n",
    "txt = []\n",
    "    \n",
    "for i in links:\n",
    "    try: \n",
    "        res = requests.get(i)\n",
    "        res.raise_for_status()\n",
    "        res.encoding = None            \n",
    "        html2 = res.text\n",
    "        \n",
    "        soup = BeautifulSoup(html2, 'html.parser')\n",
    "        contentArea = soup.find(\"div\", {\"class\" : \"viewarea\"})            \n",
    "        parags = contentArea.findAll(\"div\", {\"id\" : \"contentArea\"})\n",
    "\n",
    "        content = \"\"\n",
    "\n",
    "        for parag in parags:\n",
    "            content += parag.text\n",
    "        content = re.sub('[ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', '', content)\n",
    "        content = re.sub('[&nbsp;|\\n|\\t|\\r]', '', content)\n",
    "        content = re.sub('[\\xa0]', '', content)\n",
    "        content = re.sub('[-=+_★♥♡,#/\\?:╋^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》—;]', '', content)\n",
    "        content = re.sub('[^ ㄱ-ㅣ가-힣]','',content)\n",
    "        #content = spacing(content)\n",
    "        \n",
    "        txt.append(content)\n",
    "             \n",
    "    except HTTPError as e:\n",
    "        txt.append('')\n",
    "    except URLError as e:\n",
    "        txt.append('')\n",
    "    except AttributeError as e:\n",
    "        txt.append('')\n",
    "        \n",
    "#print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt() \n",
    "\n",
    "title_morphs = []\n",
    "txt_morphs = []\n",
    "    \n",
    "for i in titles:\n",
    "    title_morphs.append(okt.morphs(i))\n",
    "    \n",
    "for i in txt:\n",
    "    txt_morphs.append(okt.morphs(i))\n",
    "    \n",
    "\n",
    "nate_dict = {\n",
    "    '제목' : titles,\n",
    "    '본문' : txt\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(nate_dict) \n",
    "df.to_csv('natepann.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "  \n",
    "morphs_dict = {\n",
    "    '제목 형태소' : title_morphs,\n",
    "    '본문 형태소' : txt_morphs\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(morphs_dict)\n",
    "df2.to_csv('natepann_Morphs.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('natepann.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('natepann_Morphs.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
