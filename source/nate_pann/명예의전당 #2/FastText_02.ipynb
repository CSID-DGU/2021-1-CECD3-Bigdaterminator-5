{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 801/801 [00:00<00:00, 135545.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 생성\n",
      "학습 중\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "# FastText 적용을 위한 corpus 생성 및 크롤링 데이터 학습\n",
    "\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "corpus_fname = 'C:/Users/Administrator/Desktop/python/natepann.txt'\n",
    "model_fname = 'C:/Users/Administrator/Desktop/python/fasttext'\n",
    "\n",
    "\n",
    "print('corpus 생성')\n",
    "corpus = [sent.strip().split(\" \")for sent in tqdm(open(corpus_fname, 'r', encoding='utf-8').readlines())]\n",
    "\n",
    "print(\"학습 중\")\n",
    "model = FastText(corpus, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "model.save(model_fname)\n",
    "\n",
    "print('완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1671, 100)\n",
      "['라치카', '보니까', '홀리뱅', '리정', '하니까', '육성재', '모니카', '강다니엘', '이걸', '정리', '시미즈', '어린이집', '남돌', '허니제이', '서울대', '버블', '내년에', '톡선', '걸서위', '이유', '방금', '이달소', '돈이', '이채연', '민아', '둘다', '보아', '아이도', '여자친구', '해주고', '많다', '봐서', '레벨', '프라우드먼', '팬들', '올케언니는', '주연', '안유진', '영훈', '아이는', '노제', '쇼핑몰', '트레저', '이유가', '아들', '나은', '만원을', '블로그', '여돌', '이민혁', '광고', '행복하게', '하였고', '조용히', '로잘린', '안됨', '멜론', '차라리', '분은', '스토리', '추억이', '몇개', '가비', '한국으로', '오늘', '마스카라', '리아', '있던', '올해', '연예인', '위로가', '공주이다', '한국에', '우린', '보낸', '마름', '갈수록', '아이들이', '다이어트', '첫번째', '화장', '저게', '얼굴이', '우주호', '립제이', '마크', '디즈니', '시동생', '서울', '잊을', '있도록', '유튜브', '브랜드', '엠마', '피부', '실력', '미친', '사건', '노래를', '다르게']\n"
     ]
    }
   ],
   "source": [
    "# 새로운 용어에 FastText를 적용하여 주변 단어 100개 추출\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "loaded_model = FastText.load(\"C:/Users/Administrator/Desktop/python/fasttext\")\n",
    "print(loaded_model.wv.vectors.shape)\n",
    "\n",
    "Similar = []\n",
    "similar_word_list = list(loaded_model.wv.most_similar(\"스우파\", topn=100)) \n",
    "\n",
    "for i in range(len(similar_word_list)):\n",
    "    temp_str = similar_word_list[i][0].__str__()\n",
    "    Similar.append(temp_str)\n",
    "    \n",
    "print(Similar)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(Similar) \n",
    "df.to_csv('natepann_surrounding_words.csv',header=None,index=False, encoding='utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['보기', '많다', '않는', '여자', '보며', '얼굴을', '전부', '좋아하는', '내용을', '쓰고', '커서', '살이', '얼굴이', '중요한', '되어', '좋아하고', '사이가']\n"
     ]
    }
   ],
   "source": [
    "# 주변 단어 100개 중 knu 사전에 존재하는 주변 용어만 찾기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def fileopen(data):\n",
    "    \n",
    "    #파일 불러오기 \n",
    "    with open(data, 'r', encoding='UTF8') as file:\n",
    "        \n",
    "        text = file.read()\n",
    "        \n",
    "        splitdata = text.split()\n",
    "        \n",
    "        #리스트의 중첩된 부분 삭제하기 \n",
    "        splitdata = list(dict.fromkeys(splitdata))\n",
    " \n",
    "    return splitdata\n",
    "\n",
    "if __name__ == '__main__':\n",
    " \n",
    "    #파일1번 불러오기\n",
    "    NewList = fileopen('natepann_surrounding_words.txt')\n",
    "    \n",
    "    #파일2번 불러오기\n",
    "    NewList1 = fileopen('SentiWord_Dict.txt')\n",
    " \n",
    "    dif1 = list(set(NewList1) - set(NewList))\n",
    "    dif2 = list(set(NewList) - set(NewList1))\n",
    "    result = list((set(NewList)-set(dif1))-set(dif2))\n",
    "    \n",
    "    \n",
    "    # 두 파일중 공통된 요소 출력 \n",
    "    print(result)\n",
    "    \n",
    "df = pd.DataFrame(result) \n",
    "df.to_csv('natepann_inKNU.txt',header=None,index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt 파일 Knu 사전 Csv 파일로 변환\n",
    "\n",
    "import pandas as pd\n",
    "file = pd.read_csv('SentiWord_Dict.txt', delimiter = '\\t')\n",
    "\n",
    "file.to_csv('SentiWord_Dict.csv',index=False, encoding='utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          키워드  극성값\n",
      "0      가장 중요한  1.0\n",
      "1       겁이 많다 -2.0\n",
      "2   경망스러운 여자를 -1.0\n",
      "3   관심이나 좋아하는  2.0\n",
      "4      굽히지 않는  0.0\n",
      "..        ...  ...\n",
      "60    창백한 얼굴을 -1.0\n",
      "61     충격이 커서 -1.0\n",
      "62      키가 커서  1.0\n",
      "63      하지 않는 -1.0\n",
      "64     현숙한 여자  1.0\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "\n",
      "새로운 용어의 극성 추정값은 다음과 같습니다:\n",
      "0.046153846153846156\n"
     ]
    }
   ],
   "source": [
    "# Knu 사전에서 주변 단어가 포함된 키워드들을 찾고, 이들의 극성값을 통계 내서 제공\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "\n",
    "#키워드에 해당하는 행 출력, 추후 iteration 돌며 키워드 모두 test 할 것\n",
    "key = '많다|않는|여자|보며|얼굴을|좋아하는|커서|얼굴이|중요한|좋아하고|사이가'\n",
    "\n",
    "final = pd.DataFrame(columns = ['contents'])\n",
    "\n",
    "f = pd.read_csv('SentiWord_Dict.csv')\n",
    "\n",
    "is_key = f['키워드'].str.contains(key, na=False)\n",
    "f_key = f[is_key]\n",
    "#f_key = f_key.reset_index()\n",
    "#f_key\n",
    "\n",
    "file2 = pd.DataFrame(f_key) \n",
    "file2.to_csv('SentiWord_Dict.csv',index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f_key)\n",
    "print(\"\\n새로운 용어의 극성 추정값은 다음과 같습니다:\") \n",
    "print(file2['극성값'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
