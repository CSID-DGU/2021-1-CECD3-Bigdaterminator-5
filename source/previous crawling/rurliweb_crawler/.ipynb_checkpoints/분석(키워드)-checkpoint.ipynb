{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단순 최다 빈도 명사\n",
    "# word_cnt = Counter(nouns).most_common()\n",
    "# total_df = pd.DataFrame(word_cnt, columns = ['단어', '빈도수'])\n",
    "# total_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.corpus import kolaw\n",
    "from konlpy.tag import Okt\n",
    "#import nltk\n",
    "#from nltk import Text\n",
    "from collections import Counter\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "from future.utils import iteritems\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 형태소 개수: \n",
      "15754\n",
      "전체 명사 개수: \n",
      "5867\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('루리웹_엑셀확인용.csv')\n",
    "data.values\n",
    "values = \"\".join(str(i) for i in data.values)\n",
    "\n",
    "#okt = Okt()\n",
    "komoran = Komoran() \n",
    "\n",
    "#total_morphs = okt.morphs(values)\n",
    "total_morphs = komoran.morphs(values)\n",
    "\n",
    "print(\"전체 형태소 개수: \")\n",
    "print(len(total_morphs))\n",
    "\n",
    "#nouns = okt.nouns(values)\n",
    "nouns = komoran.nouns(values)\n",
    "print(\"전체 명사 개수: \")\n",
    "print(len(nouns))\n",
    "\n",
    "nouns = [noun for noun in nouns if len(noun) >= 2] #3글자 이상의 키워드를 뽑는 것도 고려해 볼 것\n",
    "\n",
    "bool_words = ['아니', '아서', '아도', '진짜', '생각', '라고', '지만', '정도', '만나', '제가', '지금', \n",
    "             '하나', '보이', '얘기', '마음', '우리', '사람', '가지', '생기', '잘못', '때문', '이랑', '문제', '네이트판']\n",
    "\n",
    "#unique_Nouns = set(nouns)\n",
    "for word in nouns:\n",
    "    if word in bool_words:\n",
    "        while word in nouns: \n",
    "            nouns.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('관련', 1.0)]\n",
      "[('유머게시판', 0.5130138080316842), ('통합', 0.5130138080316842), ('공지', 0.5130138080316842), ('추가', 0.4587488401146004)]\n",
      "[('회원', 0.4472135954999579), ('여러분', 0.4472135954999579), ('안녕하세요', 0.4472135954999579), ('명왕', 0.4472135954999579), ('문재인입니다', 0.4472135954999579)]\n",
      "[('수에즈운하', 0.5), ('길막', 0.5), ('사건', 0.5), ('채신근황', 0.5)]\n",
      "[('직접서비스', 0.5), ('주년', 0.5), ('이벤트', 0.5), ('진행중', 0.5)]\n",
      "[('태영호가', 0.5773502691896257), ('자본주의', 0.5773502691896257), ('더', 0.0), ('잘', 0.0), ('알아', 0.5773502691896257)]\n",
      "[('스가', 0.34292813780134024), ('끝없는', 0.38349278400514597), ('지지율', 0.38349278400514597), ('추락', 0.38349278400514597), ('기지개', 0.38349278400514597), ('펴는', 0.38349278400514597), ('아베', 0.38349278400514597)]\n",
      "[('백신', 0.5773502691896257), ('만개가', 0.5773502691896257), ('끝인가요', 0.5773502691896257)]\n",
      "[('실린', 0.5219214532200322), ('한미', 0.5219214532200322), ('정상회담', 0.42754413807941444), ('사진', 0.5219214532200322)]\n",
      "[('–', 0.0), ('바이든”', 0.0), ('정상회담', 0.5012278674127307), ('뒷', 0.0), ('얘기', 0.6118703395854738)]\n",
      "[('문재인에', 0.4564528148683627), ('친구비', 0.4564528148683627), ('상납하는', 0.4564528148683627), ('일본', 0.4564528148683627), ('스가', 0.40817068879941976)]\n",
      "[('이번', 0.6118703395854738), ('정상회담', 0.5012278674127307), ('이후로', 0.6118703395854738)]\n",
      "[('실상을', 0.3779644730092273), ('알게되면서', 0.3779644730092273), ('유튜브', 0.3779644730092273), ('믿지', 0.3779644730092273), ('말자고', 0.3779644730092273), ('다시한번', 0.3779644730092273), ('느낌', 0.3779644730092273)]\n",
      "[('후', 0.0), ('대한민국', 0.5773502691896257), ('이준썩', 0.5773502691896257), ('버전', 0.5773502691896257)]\n",
      "[('여기자를', 0.5773502691896257), ('꼭찝어서', 0.5773502691896257), ('질문하라고', 0.5773502691896257), ('한', 0.0), ('건', 0.0)]\n",
      "[('당신이', 0.5773502691896257), ('못', 0.0), ('다', 0.0), ('이룬꿈', 0.5773502691896257), ('이루겠습니다', 0.5773502691896257)]\n",
      "[('상원의원·주지사', 0.0), ('문대통령', 0.3779644730092273), ('조지아주', 0.3779644730092273), ('공장', 0.3779644730092273), ('방문', 0.3779644730092273), ('기뻐', 0.3779644730092273)]\n",
      "[('자료는', 0.4472135954999579), ('민식이법', 0.4472135954999579), ('떡밥', 0.4472135954999579), ('돌', 0.0), ('때마다', 0.4472135954999579), ('올려야겠다', 0.4472135954999579)]\n",
      "[('몰랐음저', 0.4472135954999579), ('사람이', 0.4472135954999579), ('저', 0.0), ('정도', 0.4472135954999579), ('폐급인지는에', 0.4472135954999579), ('자주나와서', 0.4472135954999579)]\n",
      "[('盧의', 0.4082482904638631), ('꿈', 0.0), ('위해', 0.4082482904638631), ('더', 0.0), ('긴', 0.0), ('시간', 0.4082482904638631), ('많은', 0.4082482904638631), ('사람들', 0.4082482904638631), ('노력해야', 0.4082482904638631)]\n",
      "[('수에즈운하', 0.5), ('길막', 0.5), ('사건', 0.5), ('채신근황', 0.5)]\n",
      "[('박근혜가', 0.5), ('유일하게', 0.5), ('맞는', 0.5), ('말', 0.0), ('했을', 0.5), ('때', 0.0)]\n",
      "[('이런거', 0.4082482904638631), ('보면', 0.4082482904638631), ('무슨', 0.4082482904638631), ('좌빨들이', 0.4082482904638631), ('단', 0.0), ('건줄', 0.4082482904638631), ('알겠다니깐', 0.4082482904638631)]\n",
      "[('조롱을', 0.4082482904638631), ('대문으로', 0.4082482904638631), ('한', 0.0), ('갤러리를', 0.4082482904638631), ('겜갤', 0.4082482904638631), ('위주로', 0.4082482904638631), ('찾아봄', 0.4082482904638631)]\n",
      "[('추가', 0.4587488401146003), ('교민', 0.5130138080316841), ('환송', 0.5130138080316841), ('인사', 0.5130138080316841)]\n",
      "[('유흥업소발', 0.5130138080316842), ('집단', 0.5130138080316842), ('감염', 0.5130138080316842), ('근황', 0.4587488401146004)]\n",
      "[('文대통령', 0.3779644730092273), ('이재명', 0.3779644730092273), ('되면', 0.3779644730092273), ('내가', 0.3779644730092273), ('죽는다', 0.3779644730092273), ('생각에', 0.3779644730092273), ('잠', 0.3779644730092273), ('안올', 0.3779644730092273), ('것', 0.3779644730092273)]\n",
      "[('청와대', 0.5), ('인스타그램', 0.5), ('세번째', 0.5), ('배터리', 0.5)]\n",
      "[('차에', 0.4472135954999579), ('싣고', 0.4472135954999579), ('다닌다는', 0.4472135954999579), ('삼겹살', 0.4472135954999579), ('테이블', 0.4472135954999579)]\n",
      "[('고', 0.0), ('노무현', 0.5773502691896257), ('전', 0.0), ('대통령님', 0.5773502691896257), ('취임사', 0.5773502691896257)]\n",
      "[('매수한', 0.5976532494246612), ('넥슨', 0.5976532494246612), ('근황', 0.5344353907670106)]\n",
      "[]\n",
      "[('승부조작', 0.5), ('잡아낸', 0.5), ('전설의', 0.5), ('검사', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('루리웹_엑셀확인용.csv')\n",
    "corpus = [' '.join(i[0].split(' ')[1:]) for i in data.values]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "for i, sent in enumerate(corpus):\n",
    "    print( [ (token, sp_matrix[i, word2id[token]]) for token in sent.split() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文대통령', '근황', '길막', '사건', '수에즈운하', '스가', '이런거', '정상회담', '채신근황', '추가']\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF \n",
    "#전체 CSV 파일 직접 입력 시\n",
    "\n",
    "article_data = pd.read_csv('루리웹_엑셀확인용.csv', encoding='utf-8', header= None)\n",
    "values = [' '.join(i[0].split(' ')[1:]) for i in article_data.values]\n",
    "\n",
    "as_one = ''\n",
    "for value in values:\n",
    "    as_one = as_one + ' ' + value\n",
    "words = as_one.split()\n",
    "\n",
    "counts = Counter(words)\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "#단어 빈도 정리\n",
    "word2idx = {word.encode(\"utf8\").decode(\"utf8\"): ii for ii, word in enumerate(vocab,1)}\n",
    "\n",
    "#딕셔너리로 정리\n",
    "idx2word = {ii: word for ii, word in enumerate(vocab)}\n",
    "\n",
    "#tf-idf \n",
    "tfidf = TfidfVectorizer(max_features = 10, max_df=0.95, min_df=0)\n",
    "\n",
    "#generate tf-idf term-document matrix\n",
    "A_tfidf_sp = tfidf.fit_transform(values)  #size D x V\n",
    "\n",
    "#tf-idf dictionary    \n",
    "tfidf_dict = tfidf.get_feature_names()\n",
    "print(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF 이미 한번 명사 단위로 정리 한 명단으로 처리할 시\n",
    "\n",
    "as_one = ''\n",
    "for noun in nouns:\n",
    "    as_one = as_one + ' ' + noun\n",
    "words = as_one.split()\n",
    "\n",
    "counts = Counter(words)\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "#단어 빈도 정리\n",
    "word2idx = {word.encode(\"utf8\").decode(\"utf8\"): ii for ii, word in enumerate(vocab,1)}\n",
    "\n",
    "#딕셔너리로 정리\n",
    "idx2word = {ii: word for ii, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf \n",
    "tfidf = TfidfVectorizer(max_features = 10, max_df=0.95, min_df=0)\n",
    "\n",
    "#generate tf-idf term-document matrix\n",
    "A_tfidf_sp = tfidf.fit_transform(nouns)  #size D x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['구역', '노무현', '대통령', '미국', '보호', '어린이', '이낙연', '일본', '정치', '한국']\n"
     ]
    }
   ],
   "source": [
    "#tf-idf dictionary    \n",
    "tfidf_dict = tfidf.get_feature_names()\n",
    "print(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3990, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array = A_tfidf_sp.toarray()\n",
    "tfidf_data = pd.DataFrame(data_array, columns=tfidf_dict)\n",
    "tfidf_data\n",
    "tfidf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tomotopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dcc839e95e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#토픽 모델링\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# LDAModel을 생성합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtomotopy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLDAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_cf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tomotopy'"
     ]
    }
   ],
   "source": [
    "#토픽 모델링\n",
    "# LDAModel을 생성합니다.\n",
    "import tomotopy as tp\n",
    "model = tp.LDAModel(k=20, alpha=0.1, eta=0.01, min_cf=5)\n",
    "\n",
    "for i, line in enumerate(nouns): #open('natepann.csv', encoding='utf-8')):\n",
    "    model.add_doc(line.strip().split()) \n",
    "\n",
    "model.train(0) \n",
    "print('Total docs:', len(model.docs))\n",
    "print('Total words:', model.num_words)\n",
    "print('Vocab size:', model.num_vocabs)\n",
    "\n",
    " \n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=10)\n",
    "    print('Topic #{}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-cea710982d00>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-cea710982d00>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip3 install tomotopy\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
